<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Learning Spatial-Temporal Implicit Neural Representations for Event-Guided Video Super-Resolution | Cayman theme</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Learning Spatial-Temporal Implicit Neural Representations for Event-Guided Video Super-Resolution" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="CVPR 2023" />
<meta property="og:description" content="CVPR 2023" />
<link rel="canonical" href="http://localhost:4000/cvpr23/egvsr.html" />
<meta property="og:url" content="http://localhost:4000/cvpr23/egvsr.html" />
<meta property="og:site_name" content="Cayman theme" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Learning Spatial-Temporal Implicit Neural Representations for Event-Guided Video Super-Resolution" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"CVPR 2023","headline":"Learning Spatial-Temporal Implicit Neural Representations for Event-Guided Video Super-Resolution","url":"http://localhost:4000/cvpr23/egvsr.html"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">Learning Spatial-Temporal Implicit Neural Representations for Event-Guided Video Super-Resolution</h1>
      <h2 class="project-tagline">CVPR 2023</h2>
      
      
        <a href="" class="btn">Code</a>
        <a href="" class="btn">Paper</a>
        <a href="" class="btn">Dataset</a>
      
    </header>

    <main id="content" class="main-content" role="main">
      <!-- <a href="https://www.youtube.com/watch?v=ty531p2Me7Q">
  <img src="assets/images/cvpr23/egvsr.png" alt="eres" style="width: 500""/>
</a> -->

<!-- [video](https://www.youtube.com/watch?v=ty531p2Me7Qng) -->
<p><a href="https://www.youtube.com/watch?v=ty531p2Me7Qng"><img src="https://i.328888.xyz/2023/03/16/K5vCL.png" alt="png" /></a></p>

<h1 id="abstract">Abstract</h1>

<p>Event cameras sense the intensity changes asynchronously and produce event streams with high dynamic range and low latency. This has inspired research endeavors utilizing events to guide the challenging video super-resolution (VSR) task. In this paper, we make the first attempt to address a novel problem of achieving VSR at random scales by taking advantages of the high temporal resolution property of events. This is hampered by the difficulties of representing the spatial-temporal information of events when guiding VSR. To this end, we propose a novel framework that incorporates the spatial-temporal interpolation of events to VSR in a unified framework. Our key idea is to learn implicit neural representations from queried spatial-temporal coordinates and features from both RGB frames and events. Our method contains three parts. Specifically, the Spatial-Temporal Fusion (STF) module first learns the 3D features from events and RGB frames. Then, the Temporal Filter (TF) module unlocks more explicit motion information from the events near the queried timestamp and generates the 2D features. Lastly, the Spatial Temporal Implicit Representation (STIR) module recovers the SR frame in arbitrary resolutions from the outputs of these two modules. In addition, we collect a real-world dataset with spatially aligned events and RGB frames. Extensive experiments show that our method significantly surpass the prior-arts and achieves VSR with random scales, e.g., 6.5.</p>

<h1 id="publication">Publication</h1>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{lu21cvpr,
  title={Learning Spatial-Temporal Implicit Neural Representations for Event-Guided Video Super-Resolution},
  author={Yunfan Lu, Zipeng Wang, Minjie Liu, Hongjian Wang, Lin Wang},
  journal={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022}
}
</code></pre></div></div>


      <footer class="site-footer">
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </main>
  </body>
</html>
